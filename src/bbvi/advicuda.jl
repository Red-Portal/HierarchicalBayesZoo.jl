
struct ADVICUDA{P} <: AdvancedVI.AbstractVariationalObjective
    prob     ::P
    n_samples::Int
    use_cuda ::Bool

    function ADVICUDA(prob, n_samples::Int, use_cuda ::Bool = false)
        cap = LogDensityProblems.capabilities(prob)
        if cap === nothing
            throw(
                ArgumentError(
                    "The log density function does not support the LogDensityProblems.jl interface",
                ),
            )
        end
        new{typeof(prob)}(prob, n_samples, use_cuda)
    end
end

function AdvancedVI.init(rng::Random.AbstractRNG, advi::ADVICUDA, Î», re)
    if advi.use_cuda
        seed = rand(rng, UInt32)
        CUDA.RNG(seed)
    else
        rng
    end
end

function (advi::ADVICUDA)(
    rng::Random.AbstractRNG, prob_batch, q,
)
    n_samples = advi.n_samples
    Î·s        = rand(rng, q, n_samples)
    ð”¼â„“ = sum(eachcol(Î·s)) do Î·áµ¢
        LogDensityProblems.logdensity(prob_batch, Î·áµ¢)
    end / n_samples
    â„  = entropy(q)
    ð”¼â„“ + â„
end

function AdvancedVI.estimate_gradient(
    rng         ::Random.AbstractRNG,
    adbackend   ::ADTypes.AbstractADType,
    advi        ::ADVICUDA,
    rng_mc,
    Î»           ::AbstractVector{<:Real},
    restructure,
    out         ::DiffResults.MutableDiffResult,
    batch_idx = nothing
)
    f(Î»â€²) = begin
        q_Î·_x = amortize(restructure(Î»â€²), batch_idx)
        -advi(rng_mc, advi.prob, q_Î·_x)
    end
    AdvancedVI.value_and_gradient!(adbackend, f, Î», out)

    nelbo = DiffResults.value(out)
    stat  = (elbo=-nelbo,)
    out, rng_mc, stat
end
